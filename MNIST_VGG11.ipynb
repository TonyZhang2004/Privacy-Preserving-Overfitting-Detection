{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590f628b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”µ Training Normal Model (5 epochs)\n",
      "Epoch 1/5 | Loss: 0.7242 | Acc: 97.98% | Frobenius Norm: 50.46\n",
      "Epoch 2/5 | Loss: 0.0776 | Acc: 98.46% | Frobenius Norm: 58.45\n",
      "Epoch 3/5 | Loss: 0.0512 | Acc: 98.53% | Frobenius Norm: 64.21\n",
      "Epoch 4/5 | Loss: 0.0414 | Acc: 99.12% | Frobenius Norm: 68.86\n",
      "Epoch 5/5 | Loss: 0.0372 | Acc: 98.93% | Frobenius Norm: 74.73\n",
      "\n",
      "ðŸ”´ Training Overfit Model (50 epochs)\n",
      "Epoch 1/50 | Loss: 0.5971 | Acc: 97.55% | Frobenius Norm: 51.16\n",
      "Epoch 2/50 | Loss: 0.0753 | Acc: 98.33% | Frobenius Norm: 60.05\n",
      "Epoch 3/50 | Loss: 0.0536 | Acc: 98.78% | Frobenius Norm: 66.82\n",
      "Epoch 4/50 | Loss: 0.0420 | Acc: 98.84% | Frobenius Norm: 72.99\n",
      "Epoch 5/50 | Loss: 0.0377 | Acc: 98.26% | Frobenius Norm: 78.84\n",
      "Epoch 6/50 | Loss: 0.0326 | Acc: 98.98% | Frobenius Norm: 84.10\n",
      "Epoch 7/50 | Loss: 0.0335 | Acc: 99.23% | Frobenius Norm: 89.56\n",
      "Epoch 8/50 | Loss: 0.0264 | Acc: 99.17% | Frobenius Norm: 93.68\n",
      "Epoch 9/50 | Loss: 0.0244 | Acc: 99.13% | Frobenius Norm: 97.26\n",
      "Epoch 10/50 | Loss: 0.0227 | Acc: 98.84% | Frobenius Norm: 101.09\n",
      "Epoch 11/50 | Loss: 0.0211 | Acc: 98.88% | Frobenius Norm: 105.22\n",
      "Epoch 12/50 | Loss: 0.0192 | Acc: 98.97% | Frobenius Norm: 109.12\n",
      "Epoch 13/50 | Loss: 0.0163 | Acc: 99.28% | Frobenius Norm: 112.18\n",
      "Epoch 14/50 | Loss: 0.0172 | Acc: 98.89% | Frobenius Norm: 115.52\n",
      "Epoch 15/50 | Loss: 0.0136 | Acc: 99.29% | Frobenius Norm: 118.62\n",
      "Epoch 16/50 | Loss: 0.0170 | Acc: 99.27% | Frobenius Norm: 121.82\n",
      "Epoch 17/50 | Loss: 0.0142 | Acc: 99.05% | Frobenius Norm: 124.57\n",
      "Epoch 18/50 | Loss: 0.0183 | Acc: 99.09% | Frobenius Norm: 128.23\n",
      "Epoch 19/50 | Loss: 0.0117 | Acc: 99.08% | Frobenius Norm: 130.65\n",
      "Epoch 20/50 | Loss: 0.0100 | Acc: 99.05% | Frobenius Norm: 132.32\n",
      "Epoch 21/50 | Loss: 0.0114 | Acc: 99.16% | Frobenius Norm: 134.29\n",
      "Epoch 22/50 | Loss: 0.0137 | Acc: 99.07% | Frobenius Norm: 136.78\n",
      "Epoch 23/50 | Loss: 0.0103 | Acc: 99.14% | Frobenius Norm: 139.06\n",
      "Epoch 24/50 | Loss: 0.0132 | Acc: 99.06% | Frobenius Norm: 141.46\n",
      "Epoch 25/50 | Loss: 0.0115 | Acc: 99.27% | Frobenius Norm: 143.55\n",
      "Epoch 26/50 | Loss: 0.0112 | Acc: 99.29% | Frobenius Norm: 146.12\n",
      "Epoch 27/50 | Loss: 0.0077 | Acc: 99.29% | Frobenius Norm: 147.70\n",
      "Epoch 28/50 | Loss: 0.0111 | Acc: 99.20% | Frobenius Norm: 149.94\n",
      "Epoch 29/50 | Loss: 0.0121 | Acc: 99.11% | Frobenius Norm: 152.11\n",
      "Epoch 30/50 | Loss: 0.0097 | Acc: 99.24% | Frobenius Norm: 154.19\n",
      "Epoch 31/50 | Loss: 0.0072 | Acc: 99.21% | Frobenius Norm: 155.45\n",
      "Epoch 32/50 | Loss: 0.0105 | Acc: 99.30% | Frobenius Norm: 157.57\n",
      "Epoch 33/50 | Loss: 0.0106 | Acc: 99.04% | Frobenius Norm: 159.32\n",
      "Epoch 34/50 | Loss: 0.0110 | Acc: 99.42% | Frobenius Norm: 160.94\n",
      "Epoch 35/50 | Loss: 0.0113 | Acc: 98.84% | Frobenius Norm: 162.91\n",
      "Epoch 36/50 | Loss: 0.0078 | Acc: 99.17% | Frobenius Norm: 164.20\n",
      "Epoch 37/50 | Loss: 0.0076 | Acc: 99.29% | Frobenius Norm: 165.81\n",
      "Epoch 38/50 | Loss: 0.0120 | Acc: 99.12% | Frobenius Norm: 167.64\n",
      "Epoch 39/50 | Loss: 0.0073 | Acc: 99.16% | Frobenius Norm: 168.87\n",
      "Epoch 40/50 | Loss: 0.0106 | Acc: 99.25% | Frobenius Norm: 170.01\n",
      "Epoch 41/50 | Loss: 0.0058 | Acc: 99.14% | Frobenius Norm: 171.00\n",
      "Epoch 42/50 | Loss: 0.0067 | Acc: 99.21% | Frobenius Norm: 172.32\n",
      "Epoch 43/50 | Loss: 0.0130 | Acc: 99.21% | Frobenius Norm: 174.34\n",
      "Epoch 44/50 | Loss: 0.0116 | Acc: 99.18% | Frobenius Norm: 176.12\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class VGG11(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG11, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 512), nn.ReLU(True),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def compute_frobenius_norm(model):\n",
    "    weights = [param for name, param in model.named_parameters() if 'weight' in name]\n",
    "    all_weights = torch.cat([w.flatten() for w in weights])\n",
    "    return torch.norm(all_weights, p=2).item()\n",
    "\n",
    "def train_model(model, optimizer, train_loader, test_loader, epochs=5, device='cpu'):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.to(device)\n",
    "    train_loss, test_acc, frob_norms = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        train_loss.append(total_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "                total += targets.size(0)\n",
    "        acc = correct / total\n",
    "        test_acc.append(acc)\n",
    "        frob_norms.append(compute_frobenius_norm(model))\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {train_loss[-1]:.4f} | Acc: {acc*100:.2f}% | Frobenius Norm: {frob_norms[-1]:.2f}\")\n",
    "    return train_loss, test_acc, frob_norms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Train normal model\n",
    "print(\"\\nðŸ”µ Training Normal Model (5 epochs)\")\n",
    "model_normal = VGG11()\n",
    "optimizer_normal = optim.Adam(model_normal.parameters(), lr=0.001)\n",
    "normal_loss, normal_acc, normal_frob = train_model(model_normal, optimizer_normal, train_loader, test_loader, epochs=5, device=device)\n",
    "\n",
    "# Train overfit model\n",
    "print(\"\\nðŸ”´ Training Overfit Model (50 epochs)\")\n",
    "model_overfit = VGG11()\n",
    "optimizer_overfit = optim.Adam(model_overfit.parameters(), lr=0.001)\n",
    "overfit_loss, overfit_acc, overfit_frob = train_model(model_overfit, optimizer_overfit, train_loader, test_loader, epochs=50, device=device)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, 6), normal_loss, label=\"Normal (5 epochs)\")\n",
    "plt.plot(range(1, 51), overfit_loss, label=\"Overfit (50 epochs)\")\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 6), normal_acc, label=\"Normal (5 epochs)\")\n",
    "plt.plot(range(1, 51), overfit_acc, label=\"Overfit (50 epochs)\")\n",
    "plt.title(\"Test Accuracy over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Frobenius Norms:\\nðŸ”µ Normal: {normal_frob[-1]:.2f}\\nðŸ”´ Overfit: {overfit_frob[-1]:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
